---
title: "Single-cell RNA-seq analysis of single species"
author: "Yasir Arafat Tamal"
date: "9/15/2023"
output: html_document:
    fig_height: 9
    fig_width: 11
    toc: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### **Goal**
Cellular gene expression profiles were generated from the developing leaf primordia, an embryonic tissue that gives rise to leaves, of wild-type C. hirsuta, a closely related species to the model A. thaliana. The C. hirsuta plants were grown in the soil in a greenhouse under short-day conditions (18°C in 16 hours dark and 21°C in 8 hours light), and leaves 5, 6, and 7 were collected at three weeks post-germination. Four biological replicates were collected, and the 10x Genomics scRNA-seq protocol was used to generate transcriptomic profiles at the single-cell level. The scRNA-seq datasets of C. hirsuta contain gene expression profiles of 4100, 6640, 9090, and 10760 cells with an average median of 5195 genes and an average median of 21,637 unique molecular identifier (UMI) counts per cell. The C. hirsuta reference genome contains a total of 27,773 genes.

### **Introduction to the dataset**
We will be analysing scRNA-seq data from C.hirsuta.

##### **The dataset contains:**
Total samples or observations : 30 samples

  * 10 tissues and timepoint (individuals)  
  * 3 biological replicates for each tissue and timepoint   

###### **Question**  
  * can we treat the replicates as independent observations or samples(pseudoreplication)?  
    + most statistical approaches, such as ANOVA, DEseq2 avoid pseudoreplication.
  
To analyze our dataset using NMF, we will treat the replicates as independent observations. **Why?**  
  
  * curse of dimensionality  
    + analyzing high dimensional data requires more amount of training data to build meaningful models.  
    + to avoid overfitting.  

##### **Loading the dataset and short overview**
**Loading the dataset:**  
```{r}
load('scRNAseq_dataset.RData')
data = as.matrix(scRNAseq_dataset) #assigning the loaded dataset into variable 'data' as a matrix
```

###### **Overview of the data**:  
  * samples in rows.  
  * genes or features in columns.  
```{r}
head(data)[,1:8]
```

**Dimension of the dataset:**
```{r}
dim(data)
```

### **Data pre-processing step:**  

##### **Filtering:**  

We will be removing the columns or genes contain any 0's. Indeed, low expressed genes may appear significantly different, but may not be reproducible and/or not biologically significant.  
```{r}
X = data[ , colSums(data == 0) == 0] #removing columns those contain any 0's(Zeros)
```  

Many studies remove any a priori unreliable components of the dataset. This:  

  * avoids noise from spurious and non-reproducible results.  
  * improves generalization power  of the model.  

For this dataset, we will be removing one sample considering as an outlier from the dataset assuming that due to the presence of technical or biological noise in that observation.
```{r}
X = X[-13, ] #dropping the sample(index 13) considering as outlier
fdata = X #assigning new filtered dataset to variable 'fdata' 
```

**Overview of the filtered data:**  
```{r}
head(X)[,1:5]
```  

**Dimension of the filtered data:**  
```{r}
dim(X)
```

**Format of the data:**  
```{r}
class(X)
```  

To fit the NMF model to the dataset, the input dataset needs to be transposed. The NMF model takes input matrix where features or genes are as rows and samples or observations as columns.  

**Transposing the dataset:**  
```{r}
X = t(X) #transposing
head(X)[,1:5]
```  

**Dimension of the transposed dataset:**
```{r}
dim(X)
```  

##### **Feature Scaling**  
Feature Scaling or Standardization is a step of Data Pre Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.  

When the dataset contains features highly varying in magnitudes and range, if left alone, many machine learning algorithms only take in the magnitude of features. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.  

Here we will apply **Min-Max Scaling** to scale the features between 0 and 1. As the input matrix for the NMF model has to be non-negative matrix.  

##### **Min-Max scaling:**  
$$\mathbf{X_n} = \frac{X - min(X)}{max(X) - min(X)}$$

```{r}
mat = (X - min(X))/(max(X) - min(X)) #applying min max scaling to the filtered data and assigning the scaled data to variable 'mat'
```  

**Overview of the transformed data:**  
```{r}
head(mat)[,1:5]
```  

**Range of the scaled dataset:**  
```{r}
min(mat) #minimum value of the scaled dataset
max(mat) #maximum value of the scaled dataset
```  
As expected. The features value range between 0 and 1.  

**Assigning sample label to the dataset:**  
```{r}
sample_label = sort(rep(c('L1T1', 'L1T2','L1T3', 'L1T4', 'L3T2', 'L3T3', 'L3T4', 'L5T3', 'L5T4', 'L7T4'), 3))
sample_label = sample_label[-13]
colnames(mat) = sample_label
```  

**Overview of the preprocessed dataset:**  
```{r}
head(mat)[,1:5]
```

### **Introduction to Non-negative matrix factorization(NMF)**  
**Nonnegative matrix factorization (NMF)** is a widely-used method for multivariate nonnegative data analysis, due to its ability to learn a parts-based representation.  
Given a non-negative input  matrix  ${A} \in R^{m*n}$ where each row represents a feature  and each column  represents an observation or sample and an positive integer k such that ${k} << min{(m,n)}$, NMF finds two non-negative matrices${W} \in R^{m*k}$ and ${H} \in R^{k*n}$so that  
$$\mathbf{A} \approx WH$$
Common loss functions are based on either the Frobenius norm or the Kullback-Leibler divergence. A solution to the NMF problem can be obtained by solving the following optimization problem:  
$$\mathbf{D} = \sum_{i,j}A_{i,j}log(A_{i,j}/(WH)_{i,j}) - A_{i,j} +(W,H)_{i,j} $$
At each step, W and H are updated by using the coupled divergence equations:  
$$\mathbf{H_{au}} \leftarrow H_{au}\frac{\sum_{i}W_{ia}A_{iu}/(WH)iu}{\sum_{k}W_{ka}} $$
$$\mathbf{W_{ia}} \leftarrow W_{ia}\frac{\sum_{u}H_{au}A_{iu}/(WH)iu}{\sum_{v}H_{av}} $$
where ${W} \in R^{m*k}$ is a basis matrix and ${H} \in R^{k*n}$ is a coefficient matrix and $W,H \geqslant 0$ means that all elements of W and H are non-negative. $\parallel {A - WH^T} \parallel^2_F$ is the Frobenius norm.  Due to k < m, dimension reduction is achieved and a lower dimensional representation of A in a k-dimensional space is given by H.

##### **Applications and Motivations:**  
The NMF approach has been applied successfully in several fields,including,  

  * image and pattern recognition  
  * signal processing and text mining  
  * cancer type discovery based on gene expression microarrays  
  * functional characterization of genes  
  * to predict cis-regulating elements from positional word count matrices  
  * phenotype prediction using cross-platform microarray data  

The popularity of the NMF approach derives essentially from three properties that distinguish it from standard decomposition techniques.  

  * firstly, the matrix factors are by definition nonnegative, which allows their intuitive interpretation as real underlying components within the context defined by the original data.  
  * secondly, NMF generally produces sparse results, which means that the basis and/or mixture coefficients have only a few non-zero entries. This provides a more compact and local representation, emphasizing even more the parts-based decomposition of the data.  
  * finally, unlike other decomposition methods such as SVD or ICA, NMF does not aim at finding components that are orthogonal or independent, but instead allows them to overlap.  

[Metagenes and molecular pattern discovery using matrix factorization](https://www.ncbi.nlm.nih.gov/pubmed/15016911)
Brunet, Jean-Philippe, et al. implemented NMF for gene expression data analysis.  
They demonstrated the ability of NMF to recover meaningful biological information from cancer-related microarray data.  

For this task, we will implement CRAN - Package NMF - The R Project for Statistical Computing.  

##### **Dependency**  
```{r echo = T, warning = F, include = FALSE}
library(NMF)
library(gtools)
library(ggplot2)
library(ggpubr)
```

##### **NMF algorithms**    
```{r echo=FALSE}
nmfAlgorithm()
```
##### **NMF seeding methods**  
```{r echo = FALSE}
nmfSeed()
```

Implementaion of the NMF algorithm:  
We will be fit NMF model to the dataset using "Brunet" algorithm and two different seeding method, "random" and "nndsvd"
```{r}
#mdl = nmf(mat, rank = 10, 'Brunet', seed = 'nndsvd', nrun = 1000, .opt = 'vP12')
#mdl = nmf(mat, rank = 10, 'Brunet', seed = 'random', nrun = 1000, .opt = 'vP12')
```
**Here,**  

  * "mat" is the input matrix.  
  * "rank" or "metagenes" is the desired rank or number of subgroups to identify.  
  * "Brunet" is the algothim used for matrix factorization.  
  * "Nonnegative Double Singular Value Decomposition(nndsvd)" is the initialization method(w, H) for the factor matrix. NNDSVD is a method designed to enhance the initialization stage of NMF.  
  * "random" is the initialization method(w, H) for the factor matrix.
  * "nrun = 1000" is the number of multiple runs performed for matrix factorization to achieve stability.  
  * ".opt = 'vP12'" which turns on verbosity at level 1, that will show which parallel setting is used by each computation. Using option 'P'  force parallel computation to a certain number of cores.     

##### **Computational speed:**  
Performing a single NMF run on large scale data requires intensive computations. Moreover, a typical NMF analysis involves performing several runs for different values of the rank (~ 30-50 runs), before running the final factorization using the estimated rank (~ 200 runs) The whole procedure is therefore highly time consuming.  
For this purpose, here we will be using already implemeted NMF model that was fitted on our dataset as mentioned above, which was saved as a R object.  
```{r}
load('classifier_nndsvd.RData')
classifierNndsvd = classifier_brunet_nndsvdSeed

load('classifier_random.RData')
classifierRandom = classifier_brunet_randomSeed
```

**Short summary of the fitted NMF model:**  
```{r}
classifierRandom
```


##### **Basis matrix and coefficient matrix:**  
```{r}
# the generic functions "basis()" and "coef()" extracts and set the matrix of basis components of an NMF model
W = basis(classifierRandom) #basis components
H = coef(classifierRandom) #mixture coefficients
```


The **Basis matrix(W)** is the metagene composition, which represents the contribution of the features to the metagenes or ranks.  
```{r}
head(W)[, 1:5]
```


The **coefficient matrix(H)** is the metagene expression profile which represents the sample assignment depending on the relative
values in each column of H.   
```{r}
head(H)[, 1:5]
```


##### **Visualizing results:**  
The NMF package includes a wide range of powerful plotting utilities. To interpret and evaluate the estimated factorization, NMF package implements a collection of functions pre-configured to visualize the results from NMF runs. Each visualization method provides insights about specific characteristics of the result or the method used.  
One of the main properties of NMF is its ability to produce metagenes or metagene expression profiles that have a sparse structure. The following figures show the heatmaps generated from the metagene expression profiles (H) and metagene matrices (W) respectively. Both matrices were obtained from the same factorization, that is the one that achieved the lowest approximation error across 150 runs of the Brunet et al.'s algorithm.  
High values are displayed in red, small values in yellow.  


##### **Coefficient matrix(H):**  
```{r echo = F}
par(mfrow = c(1,2))
coefmap(classifierRandom, Colv = 'basis', Rowv = NA, main = 'Random seeding')
coefmap(classifierNndsvd, Colv = 'basis', Rowv = NA, main = 'NNDSVD seeding')
mtext("Contribution of metagenes on the samples (Coefficient Matrix H)", outer = TRUE,  cex = 0.8, line = -0.8)
```
Each column corresponds to a samples. The top colored row shows the phenotypic class to which each sample belongs. To emphasize the relative contribution of each metagene to each sample, the columns are scaled to sum to one and ordered by clusters, which are highlighted on the second row by colours that map them with their associated metagene.  


##### **Basis matrix(W):**  
The metagene matrix, W, contains the contribution of several thousand genes to each metagene.  
```{r echo = F}
par(mfrow = c(1,2))
basismap(classifierRandom, Colv = NA, Rowv = NA, main = 'Random seeding')
basismap(classifierNndsvd, Colv = NA, Rowv = NA, main = 'NNDSVD seeding')
mtext("Contribution of features on the ranks (Basis Matrix W)", outer = TRUE,  cex = 0.8, line = -0.8)
```
Due to its sparse structure, a heatmap of W showing all the genes is usually not easily interpretable. Each row corresponds to a gene and each column corresponds to a metagene or rank. The most metagene-specific genes were selected using the Kim and Park's scoring and filtering method. Rows were scaled to sum to one.  

The fitted NMF model performs differently with different seeding method. Initialization of the factor matrix using **nndsvd** seeding method results overlapping of the sample groups in the same rank and could only group the samples in 7 groups. In contrast, random initialization of the factor matrix using **random** seeding method performs better than the nndsvd and groups the sample in 10 groups.
```{r}
print(paste('Residuals for seeding method nndsvd', classifierNndsvd@residuals), quote = FALSE)
print(paste('Residuals for seeding method random',classifierRandom@residuals), quote = FALSE)
```
For further analysis we will use the fitted NMF model with "random" sedding.  

##### **Feature Selection in NMF Models:**  


**Feature Score:**  
The function **"featureScore"** can compute basis-specificity scores using the "kim"(default) method defined by Kim et al. (2007).  
The function returns a numeric vector of the length the number of rows in Basis matrix´with one score per feature.  
The score for feature i is defined as:
$$\mathbf{Score\_i} = 1 + \frac{1}{log_2(k)} sum\_q[p(i,q) log2( p(i,q)) ]$$
where p(i,q) is the probability that the $i_{th}$ feature contributes to basis q:
$$\mathbf{p(i,q) = \frac{W(i,q)}{sum\_r W(i,r)}}$$
The feature scores are real values within the range [0,1]. The higher the feature score the more basis-specific the corresponding feature.

```{r}
feature_score = featureScore(classifierRandom, method = "kim") #computes basis-specificity scores for each feature in the data
top_features = sort(feature_score, decreasing = TRUE)
top_20_features = sort(feature_score, decreasing = TRUE)[1:20] #overall top 20 features
feature_names = mixedsort(names(top_20_features)) #sorting the feature names
feature_names[1:10]
```

```{r}
top_20_features[1:10]
```

**dataset with these top 20 features:**  
```{r}
dataset_with_top20_features = fdata[, feature_names] #selecting only top 20 features(columns) from the original raw 
samples = c(1:29)
y = sort(rep(c(1:10), 3))
y = y[-13]
new_df = cbind(dataset_with_top20_features, samples, y)
new_df = t(new_df)
colnames(new_df) = sample_label
new_df = as.data.frame(t(new_df))
```


**overview of the created dataset:**  
```{r}
dataset_with_top20_features[1:6, 1:6]
```
The read count is really low for these genes. For all these genes there read count is 0 for most of the samples except few.  

Plotting the count data profile for these genes based on their feature score(basis specificity):  
```{r echo = F}

p1 = ggplot(new_df, aes(x = samples, y = new_df[, names(top_20_features[1])], color = as.factor(y))) + geom_point() + geom_text(label = sample_label, size = 3, nudge_x = .8, nudge_y = .8) + labs(color = 'Strength', y = names(top_20_features[1])) + scale_color_hue(l = 40, c = 35)
p2 = ggplot(new_df, aes(x = samples, y = new_df[, names(top_20_features[2])], color = as.factor(y))) + geom_point() + geom_text(label = sample_label, size = 3, nudge_x = .8, nudge_y = .8) + labs(color = 'Strength', y = names(top_20_features[2])) + scale_color_hue(l = 40, c = 35)
p3 = ggplot(new_df, aes(x = samples, y = new_df[, names(top_20_features[3])], color = as.factor(y))) + geom_point() + geom_text(label = sample_label, size = 3, nudge_x = .8, nudge_y = .8) + labs(color = 'Strength', y = names(top_20_features[3])) + scale_color_hue(l = 40, c = 35)
p4 = ggplot(new_df, aes(x = samples, y = new_df[, names(top_20_features[4])], color = as.factor(y))) + geom_point() + geom_text(label = sample_label, size = 3, nudge_x = .8, nudge_y = .8) + labs(color = 'Strength', y = names(top_20_features[4])) + scale_color_hue(l = 40, c = 35)
p5 = ggplot(new_df, aes(x = samples, y = new_df[, names(top_20_features[5])], color = as.factor(y))) + geom_point() + geom_text(label = sample_label, size = 3, nudge_x = .8, nudge_y = .8) + labs(color = 'Strength', y = names(top_20_features[5])) + scale_color_hue(l = 40, c = 35)
p6 = ggplot(new_df, aes(x = samples, y = new_df[, names(top_20_features[6])], color = as.factor(y))) + geom_point() + geom_text(label = sample_label, size = 3, nudge_x = .8, nudge_y = .8) + labs(color = 'Strength', y = names(top_20_features[6])) + scale_color_hue(l = 40, c = 35)
ggarrange(p1, p2, p3, p4, p5, p6, nrow = 2, ncol = 3)
```

```{r echo = F}
par(mfrow = c(2,3))
for (i in 14:19){
  plot(dataset_with_top20_features[, names(top_20_features)[i]], ylab = names(top_20_features)[i], xlab = 'Samples')
  text(dataset_with_top20_features[, names(top_20_features)[i]], sample_label, cex = 0.8, pos = 4, col = "red")
}
```

Plotting the contribution profile for these genes based on the basis matrix W:
```{r echo = F}
par(mfrow = c(2,3))
for (i in 1:6){
  barplot(W[names(top_20_features)[i], ], ylab = names(top_20_features)[i], xlab = 'Ranks')
}
```

##### **Feature Selection:**  
The function **"extractFeatures"** implements "kim" method to select the most basis-specific features of each basis component.  

"kim" uses Kim et al. (2007) scoring schema and feature selection method. The features are first scored using the function **featureScore** with method "kim". Then only the features that fulfil both following criteria are retained:  

  * score greater than $\hat{\mu} + 3 \hat{\sigma}$ , where $\hat{\mu}$ and $\hat{\sigma}$ are the median and the median absolute deviation (MAD) of the scores respectively;  
  * the maximum contribution to a basis component is greater than the median of all contributions (i.e. of all elements of W).  

```{r}
extract_features = extractFeatures(classifierRandom,  200L) #extracting top 50 basis specific features for each basis rank or metagene
str(extract_features) #shwoing the list of genes for 10 ranks
```

Now lets extract features for each basis component.  
```{r}
# extracting top 50 genes for each basis component or rank, those contributed the most on each basis component
rank_1_genes = colnames(fdata[, extract_features[[1]]])
rank_2_genes = colnames(fdata[, extract_features[[2]]])
rank_3_genes = colnames(fdata[, extract_features[[3]]])
rank_4_genes = colnames(fdata[, extract_features[[4]]])
rank_5_genes = colnames(fdata[, extract_features[[5]]])
rank_6_genes = colnames(fdata[, extract_features[[6]]])
rank_7_genes = colnames(fdata[, extract_features[[7]]])
rank_8_genes = colnames(fdata[, extract_features[[8]]])
rank_9_genes = colnames(fdata[, extract_features[[9]]])
rank_10_genes = colnames(fdata[, extract_features[[10]]])

# top contributed genes for rank 3
rank_3_genes[1:10]
```

##### **Visualization results:**  
Plotting the count data profile for these genes:
```{r}
par(mfrow = c(2,3))
for (i in 1:6){
  plot(fdata[,rank_3_genes[i]], ylab = rank_3_genes[i], xlab = 'Samples', main = 'Rank 3 - Leaf5_TimePoint3')
  text(fdata[,rank_3_genes[i]], sample_label, cex = 0.8, pos = 3, col = "red")
}
```
It is difficult to interpret the result from the most significant genes for each rank. This is due to overlapping of many genes for all these ranks.  

**Overlapped genes for the ranks:**
```{r}
overlapped_genes = intersect(rank_3_genes, rank_1_genes) #same genes are present in rank 3 and rank 1
length(overlapped_genes) #number of genes overlapped in these two ranks
```

Plotting the count data profile for these overlapped genes from rank 3 and rank 1:  
```{r}
par(mfrow = c(2,3))
for (i in 1:6){
  plot(fdata[,overlapped_genes[i]], ylab = overlapped_genes[i], xlab = 'Samples', main = 'Rank 3 & Rank 1')
  text(fdata[,overlapped_genes[i]], sample_label, cex = 0.8, pos = 3, col = "red")
}
```
These overlapped genes do not show any distinct information for the particular rank.  

**Unique genes for the ranks:**  
```{r}
#feature set contains all the top contributed genes on  each rank
feature_set = cbind(rank_1_genes, rank_2_genes, rank_3_genes, rank_4_genes, rank_5_genes, rank_6_genes, rank_7_genes, rank_8_genes, rank_9_genes, rank_10_genes)

#genes those are only present in rank 3
unique_genes_rank3 = rank_3_genes[!match(feature_set[, 3], unique(c(feature_set[, -3])), nomatch = 0)]
unique_genes_rank3
```

Plotting the count data profile for the unique genes found in Rank 3:
```{r}
par(mfrow = c(2,3))
for (i in 1:6){
  plot(fdata[,unique_genes_rank3[i]], ylab = unique_genes_rank3[i], xlab = 'Samples', main = 'Rank 3 - Leaf5_TimePoint3')
  text(fdata[,unique_genes_rank3[i]], sample_label, cex = 0.8, pos = 3, col = "red")
}
```
The unique gene set found from each rank shows much interesting result that it is easier to differentiate the rank from other ranks.

### **Conclusion**

Single-cell RNA-Sequencing (scRNA-Seq) is a fast-evolving technology that enables the understanding of biological processes at an unprecedentedly high resolution. However, well-suited bioinformatics tools to analyze the data generated from this new technology are still lacking.  
Recently, various methods have been developed to detect subpopulations (or sub-clusters) within a group of cells using scRNA-Seq data, including scLVM (Buettner et al., 2015), BackSpin (Zeisel et al., 2015), PAGODA (Fan et al., 2016), and SEURAT (Macosko et al., 2015). These new computational tools are evidence that understanding scRNA-Seq heterogeneity is of paramount importance. Moreover, once the subpopulations are identified, it is very crucial to find the gene expression signatures that are characteristic of each subpopulation (subclass), in order to reveal the subline biological mechanisms. Here we have identified the most significant genes associated with different sub-groups in terms of basis components or ranks using non-negative matrix factorization(NMF).  
Previously, NMF has been applied to other areas in computational biology, such as molecular pattern discovery (Brunet et al., 2004), class comparison and prediction (Gao & Church, 2005), cross-platform and cross-species analysis (Tamayo et al., 2007), and identification of subpopulations of cancer patients with mutations in similar network regions (ref). More recently, NMF has been applied to gene expression profiling studies at the population level (Qi et al., 2009). Compared to other methods, NMF showed multiple advantages, such as less sensitivity to a priori selection of genes or initial conditions and the ability to detect context-dependent patterns of gene expression (Rajapakse, Tan & Rajapakse, 2004). Based on these properties, we hypothesize that NMF is less prone to the influence of noise in the scRNA-Seq data, and thus it can detect a group of genes that robustly differentiate single cells from different subpopulations.  

### **References**
[NMF package R](http://cran.r-project.org/package=NMF)  
[R project NMF](https://r-forge.r-project.org/projects/nmf)   
[Metagenes and molecular pattern discovery using matrix factorization](Brunet, J. P., Tamayo, P., Golub, T. R., & Mesirov, J. P. (2004). Metagenes and molecular pattern discovery using matrix factorization. Proceedings of the national academy of sciences, 101(12), 4164-4169.)  
[Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method.](Kim, H., & Park, H. (2008). Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method. SIAM journal on matrix analysis and applications, 30(2), 713-730.)
[Detecting heterogeneity in single-cell RNA-Seq data by non-negative matrix factorization.](Zhu, X., Ching, T., Pan, X., Weissman, S. M., & Garmire, L. (2017). Detecting heterogeneity in single-cell RNA-Seq data by non-negative matrix factorization. PeerJ, 5, e2888.)  
[Sparse nonnegative matrix factorization for clustering.](Kim, J., & Park, H. (2008). Sparse nonnegative matrix factorization for clustering. Georgia Institute of Technology.)  
[A framework for regularized non-negative matrix factorization, with application to the analysis of gene expression data.](Taslaman, L., & Nilsson, B. (2012). A framework for regularized non-negative matrix factorization, with application to the analysis of gene expression data. PloS one, 7(11), e46331.)  
[Nonnegative matrix factorization: an analytical and interpretive tool in computational biology.](Devarajan, K. (2008). Nonnegative matrix factorization: an analytical and interpretive tool in computational biology. PLoS computational biology, 4(7), e1000029.)  
[Fast and efficient estimation of individual ancestry coefficients.](Frichot, E., Mathieu, F., Trouillon, T., Bouchard, G., & François, O. (2014). Fast and efficient estimation of individual ancestry coefficients. Genetics, 196(4), 973-983.)
[A review of nonnegative matrix factorization methods for clustering.](Türkmen, A. C. (2015). A review of nonnegative matrix factorization methods for clustering. arXiv preprint arXiv:1507.03194.)  
[Robust classification of single-cell transcriptome data by nonnegative matrix factorization.](Shao, C., & Höfer, T. (2017). Robust classification of single-cell transcriptome data by nonnegative matrix factorization. Bioinformatics, 33(2), 235-242.)  
[A flexible R package for nonnegative matrix factorization.](Gaujoux, R., & Seoighe, C. (2010). A flexible R package for nonnegative matrix factorization. BMC bioinformatics, 11(1), 367.)  
[Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis.](Kim, H., & Park, H. (2007). Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis. Bioinformatics, 23(12), 1495-1502.)
[SVD based initialization: A head start for nonnegative matrix factorization.](Boutsidis, C., & Gallopoulos, E. (2008). SVD based initialization: A head start for nonnegative matrix factorization. Pattern Recognition, 41(4), 1350-1362.)  
[Algorithms and applications for approximate nonnegative matrix factorization.](Berry, M. W., Browne, M., Langville, A. N., Pauca, V. P., & Plemmons, R. J. (2007). Algorithms and applications for approximate nonnegative matrix factorization. Computational statistics & data analysis, 52(1), 155-173.)  
